# -*- coding: utf-8 -*-
"""Mental_Health_Chatbot_GEN_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U_t3iMLt-NXEp9k7tmD4MbDFBGng323c
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
result = llm.invoke("Who is lord Ram?")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install sentence_transformers

!pip uninstall my_module
!pip install my_module

!pip install -U langchain-groq

!pip install langchain_groq langchain_core langchain_community

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq


import os
def initialize_llm():
  llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model_name = "llama-3.3-70b-versatile"
)
  return llm

def create_vector_db():
  loader = DirectoryLoader("/content/data/", glob = '*.pdf', loader_cls = PyPDFLoader)
  documents = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 50)
  texts = text_splitter.split_documents(documents)
  embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
  vector_db = Chroma.from_documents(texts, embeddings, persist_directory = './chroma_db')
  vector_db.persist()

  print("ChromaDB created and data saved")

  return vector_db

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever()
  prompt_templates = """ You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """
  PROMPT = PromptTemplate(template = prompt_templates, input_variables = ['context', 'question'])

  qa_chain = RetrievalQA.from_chain_type(
      llm = llm,
      chain_type = "stuff",
      retriever = retriever,
      chain_type_kwargs = {"prompt": PROMPT}
  )
  return qa_chain


def main():
  print("Intializing Chatbot.........")
  llm = initialize_llm()

  db_path = "/content/chroma_db"

  if not os.path.exists(db_path):
    vector_db  = create_vector_db()
  else:
    embeddings = HuggingFaceBgeEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)
  qa_chain = setup_qa_chain(vector_db, llm)

  while True:
    query = input("\nHuman: ")
    if query.lower()  == "exit":
      print("Chatbot: Take Care of yourself, Goodbye!")
      break
    response = qa_chain.run(query)
    print(f"Chatbot: {response}")

if __name__ == "__main__":
  main()

create_vector_db()

!pip install gradio

!pip install langchain-groq

!pip install langchain-groq gradio chromadb sentence-transformers

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
import os
import gradio as gr

# Initialize LLM with Groq
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",  # ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”
        model_name="llama2-70b-4096"  # Groqì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë¸ëª…ìœ¼ë¡œ ë³€ê²½
    )
    return llm

# Vector Database Creation
def create_vector_db():
    loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)

    embeddings = HuggingFaceBgeEmbeddings(
        model_name='sentence-transformers/all-MiniLM-L6-v2'
    )

    vector_db = Chroma.from_documents(
        texts,
        embeddings,
        persist_directory='./chroma_db'
    )
    vector_db.persist()

    print("ChromaDB created and data saved")
    return vector_db

# Setup QA Chain
def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_templates = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
    {context}
    User: {question}
    Chatbot: """

    PROMPT = PromptTemplate(
        template=prompt_templates,
        input_variables=['context', 'question']
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# Chatbot Response Function
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history
    try:
        response = qa_chain.run(user_input)
        history.append((user_input, response))
        return "", history
    except Exception as e:
        return f"Error: {str(e)}", history

# Main Execution
print("Initializing Chatbot.........")
try:
    llm = initialize_llm()

    db_path = "/content/chroma_db"

    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceBgeEmbeddings(
            model_name='sentence-transformers/all-MiniLM-L6-v2'
        )
        vector_db = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )

    qa_chain = setup_qa_chain(vector_db, llm)

    # Gradio Interface
    with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
        gr.Markdown("# ğŸ§  Mental Health Chatbot ğŸ¤–")
        gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

        chatbot = gr.ChatInterface(
            fn=chatbot_response,
            title="Mental Health Chatbot"
        )

        gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

    app.launch()

except Exception as e:
    print(f"Error during initialization: {str(e)}")

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq
import os
import gradio as gr
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize LLM with Groq
def initialize_llm():
    try:
        llm = ChatGroq(
            temperature=0.7,  # ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ë†’ì´ê¸° ìœ„í•´ temperature ì¡°ì •
            groq_api_key="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",
            model_name="mixtral-8x7b-32768"  # ëª¨ë¸ ë³€ê²½
        )
        logger.info("LLM initialized successfully")
        return llm
    except Exception as e:
        logger.error(f"Error initializing LLM: {str(e)}")
        raise

# Vector Database Creation
def create_vector_db():
    try:
        if not os.path.exists("/content/data"):
            os.makedirs("/content/data")
            logger.warning("Created /content/data directory as it didn't exist")

        loader = DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
        documents = loader.load()

        if not documents:
            logger.warning("No documents found in the specified directory")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        texts = text_splitter.split_documents(documents)

        embeddings = HuggingFaceBgeEmbeddings(
            model_name='sentence-transformers/all-MiniLM-L6-v2'
        )

        vector_db = Chroma.from_documents(
            texts,
            embeddings,
            persist_directory='./chroma_db'
        )
        vector_db.persist()

        logger.info("ChromaDB created and data saved successfully")
        return vector_db
    except Exception as e:
        logger.error(f"Error creating vector database: {str(e)}")
        raise

# Setup QA Chain
def setup_qa_chain(vector_db, llm):
    try:
        retriever = vector_db.as_retriever(search_kwargs={"k": 3})  # ìƒìœ„ 3ê°œ ê²°ê³¼ ê²€ìƒ‰
        prompt_templates = """You are a compassionate mental health chatbot. Based on the following context, provide a helpful and empathetic response:

        Context: {context}

        Question: {question}

        Please provide a detailed and supportive answer:"""

        PROMPT = PromptTemplate(
            template=prompt_templates,
            input_variables=['context', 'question']
        )

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT}
        )
        logger.info("QA Chain setup completed")
        return qa_chain
    except Exception as e:
        logger.error(f"Error setting up QA chain: {str(e)}")
        raise

# Chatbot Response Function
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history

    try:
        logger.info(f"Processing user input: {user_input}")
        response = qa_chain.run(user_input)

        if not response:
            response = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"

        history.append((user_input, response))
        return "", history
    except Exception as e:
        logger.error(f"Error in chatbot response: {str(e)}")
        return f"I apologize, but an error occurred: {str(e)}", history

# Main Execution
def main():
    print("Initializing Chatbot.........")
    try:
        global qa_chain  # Make qa_chain global so it can be accessed by chatbot_response
        llm = initialize_llm()

        db_path = "/content/chroma_db"

        if not os.path.exists(db_path):
            vector_db = create_vector_db()
            if vector_db is None:
                raise Exception("Failed to create vector database")
        else:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )
            vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )

        qa_chain = setup_qa_chain(vector_db, llm)

        # Gradio Interface
        with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
            gr.Markdown("# ğŸ§  Mental Health Chatbot ğŸ¤–")
            gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

            chatbot = gr.ChatInterface(
                fn=chatbot_response,
                title="Mental Health Chatbot",
                examples=["How can I manage stress?", "What are some relaxation techniques?"],
                retry_btn="Retry",
                undo_btn="Undo"
            )

            gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

        app.launch(share=True, debug=True)

    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}")
        raise

if __name__ == "__main__":
    main()

# Gradio Interface ë¶€ë¶„ ìˆ˜ì •
def main():
    print("Initializing Chatbot.........")
    try:
        global qa_chain
        llm = initialize_llm()

        db_path = "/content/chroma_db"

        if not os.path.exists(db_path):
            vector_db = create_vector_db()
            if vector_db is None:
                raise Exception("Failed to create vector database")
        else:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )
            vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )

        qa_chain = setup_qa_chain(vector_db, llm)

        # Gradio Interface - ìˆ˜ì •ëœ ë¶€ë¶„
        with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
            gr.Markdown("# ğŸ§  Mental Health Chatbot ğŸ¤–")
            gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

            chatbot = gr.ChatInterface(
                fn=chatbot_response,
                title="Mental Health Chatbot",
                examples=["How can I manage stress?", "What are some relaxation techniques?"]
            )

            gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

        app.launch(share=True, debug=True)

    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}")
        raise

if __name__ == "__main__":
    main()

# Chatbot Response Function ìˆ˜ì •
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history

    try:
        logger.info(f"Processing user input: {user_input}")
        response = qa_chain.run(user_input)

        if not response:
            response = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"

        # ë°˜í™˜ í˜•ì‹ ìˆ˜ì •
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": response})
        return response, history
    except Exception as e:
        logger.error(f"Error in chatbot response: {str(e)}")
        error_message = f"I apologize, but an error occurred: {str(e)}"
        history.append({"role": "assistant", "content": error_message})
        return error_message, history

# Gradio Interface ìˆ˜ì •
def main():
    print("Initializing Chatbot.........")
    try:
        global qa_chain
        llm = initialize_llm()

        db_path = "/content/chroma_db"

        if not os.path.exists(db_path):
            vector_db = create_vector_db()
            if vector_db is None:
                raise Exception("Failed to create vector database")
        else:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )
            vector_db = Chroma(
                persist_directory=db_path,
                embedding_function=embeddings
            )

        qa_chain = setup_qa_chain(vector_db, llm)

        # Gradio Interface
        with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
            gr.Markdown("# ğŸ§  Mental Health Chatbot ğŸ¤–")
            gr.Markdown("A compassionate chatbot designed to assist with mental well-being. Please note: For serious concerns, contact a professional.")

            chatbot = gr.Chatbot()
            msg = gr.Textbox(label="Type your message here...")
            clear = gr.Button("Clear")

            def user(user_message, history):
                return "", history + [[user_message, None]]

            def bot(history):
                user_message = history[-1][0]
                response, _ = chatbot_response(user_message, [])
                history[-1][1] = response
                return history

            msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
                bot, chatbot, chatbot
            )
            clear.click(lambda: None, None, chatbot, queue=False)

            gr.Markdown("This chatbot provides general support. For urgent issues, seek help from licensed professionals.")

        app.launch(share=True, debug=True)

    except Exception as e:
        logger.error(f"Error during initialization: {str(e)}")
        raise

if __name__ == "__main__":
    main()

!pip install langchain-core

!pip install langchain langchain-groq chromadb sentence-transformers

!pip install -U langchain langchain-groq chromadb sentence-transformers gradio

!pip install langchain langchain-groq chromadb sentence-transformers gradio

GROQ_API_KEY = "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"

!pip install --upgrade gradio

import os
import logging
import gradio as gr
from langchain_groq import ChatGroq
from langchain.memory import ConversationSummaryMemory
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.vectorstores import Chroma

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Groq API í‚¤ ì„¤ì •
GROQ_API_KEY = "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"
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

class ChatbotWithMemory:
    def __init__(self):
        self.llm = None
        self.memory = None
        self.qa_chain = None
        self.initialize_components()

    def initialize_components(self):
        try:
            # LLM ì´ˆê¸°í™”
            self.llm = ChatGroq(
                temperature=0.7,
                groq_api_key=GROQ_API_KEY,
                model_name="mixtral-8x7b-32768"
            )

            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            self.memory = ConversationSummaryMemory(
                llm=self.llm,
                memory_key="chat_history",
                return_messages=True
            )

            # Vector DB ì´ˆê¸°í™”
            self.vector_db = self.create_vector_db()

            # QA Chain ì´ˆê¸°í™”
            self.qa_chain = self.setup_qa_chain()

        except Exception as e:
            logger.error(f"Initialization error: {str(e)}")
            raise

    def create_vector_db(self):
        try:
            embeddings = HuggingFaceBgeEmbeddings(
                model_name='sentence-transformers/all-MiniLM-L6-v2'
            )

            texts = [
                "ì •ì‹  ê±´ê°•ì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.",
                "ë„ì›€ì„ ìš”ì²­í•˜ëŠ” ê²ƒì€ ìš©ê¸°ì˜ í‘œí˜„ì…ë‹ˆë‹¤.",
                "ìƒë‹´ì€ ë§¤ìš° íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]

            db_path = "./chroma_db"
            if not os.path.exists(db_path):
                vector_db = Chroma.from_texts(
                    texts=texts,
                    embedding=embeddings,
                    persist_directory=db_path
                )
            else:
                vector_db = Chroma(
                    persist_directory=db_path,
                    embedding_function=embeddings
                )

            return vector_db
        except Exception as e:
            logger.error(f"Vector DB creation error: {str(e)}")
            raise

    def setup_qa_chain(self):
        try:
            retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})

            # ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = PromptTemplate(
                template="""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

                ì»¨í…ìŠ¤íŠ¸: {context}
                ì´ì „ ëŒ€í™”: {chat_history}
                ì§ˆë¬¸: {question}

                ë‹µë³€:""",
                input_variables=["context", "chat_history", "question"]
            )

            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={
                    "prompt": prompt,
                    "verbose": True
                }
            )

            return qa_chain
        except Exception as e:
            logger.error(f"QA Chain setup error: {str(e)}")
            raise

    def get_response(self, user_input, history):
        try:
            if not user_input.strip():
                return history

            # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

            # QA Chain ì‹¤í–‰
            response = self.qa_chain({
                "question": user_input,
                "chat_history": str(chat_history)
            })

            answer = response.get('result', "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ëŒ€í™” ì €ì¥
            self.memory.save_context(
                {"input": user_input},
                {"output": answer}
            )

             # messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            if history is None:
                history = []
            history.append(
                {"role": "user", "content": user_input}
            )
            history.append(
                {"role": "assistant", "content": answer}
            )

            return history

        except Exception as e:
            logger.error(f"Response generation error: {str(e)}")
            if history is None:
                history = []
            history.append(
                {"role": "user", "content": user_input}
            )
            history.append(
                {"role": "assistant", "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
            )
            return history

    def clear_history(self):
        self.memory.clear()
        return None

def create_gradio_interface():
    chatbot = ChatbotWithMemory()

    with gr.Blocks(theme=gr.themes.Soft()) as app:
        gr.Markdown("# ğŸ¤– Mental Health Chatbot")

        chat_interface = gr.Chatbot(
            show_label=True,
            height=600,
            type="messages"  # messages í˜•ì‹ ì‚¬ìš©
        )

        with gr.Row():
            user_input = gr.Textbox(
                show_label=False,
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                container=False
            )
            submit_btn = gr.Button("ì „ì†¡", variant="primary")

        clear_btn = gr.Button("ëŒ€í™” ë‚´ìš© ì§€ìš°ê¸°")

        def user(user_message, history):
            if history is None:
                history = []
            history.append({"role": "user", "content": user_message})
            return "", history

        def bot(history):
            if not history:
                return history
            last_user_message = [msg for msg in history if msg["role"] == "user"][-1]["content"]
            history = chatbot.get_response(last_user_message, history)
            return history

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
        user_input.submit(
            user,
            [user_input, chat_interface],
            [user_input, chat_interface]
        ).then(
            bot,
            chat_interface,
            chat_interface
        )

        submit_btn.click(
            user,
            [user_input, chat_interface],
            [user_input, chat_interface]
        ).then(
            bot,
            chat_interface,
            chat_interface
        )

        clear_btn.click(
            lambda: None,
            None,
            chat_interface,
            queue=False
        )

    return app
def main():
    print("Starting Chatbot Application...")
    try:
        app = create_gradio_interface()
        app.launch(debug=True, share=True)
    except Exception as e:
        logger.error(f"Application startup error: {str(e)}")
        raise

if __name__ == "__main__":
    main()

